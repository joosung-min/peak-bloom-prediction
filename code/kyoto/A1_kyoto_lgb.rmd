---
title: "Peak Cherry Blossom Prediction in Kyoto 2023"
author: "Joosung (Sonny) Min"
output: html_document
---

In this document, we perform LightGBM using the weather data from the closest cities to Kyoto, Japan. 

1. Select 10 most closest cities to Kyoto to increase our sample size.

    - We use 5 most recent bloom_doy along with the geographical location (lat, long, alt) of all cities listed in Japan on data/japan.csv.

    - Perform PCA and compute the Euclidean distances between the cities, and select top 10 cities with the smallest distance to Kyoto.

2. Compute the accumulated chill days and anti-chill days (Cd_cumsum, Ca_cumsum) 
    
    - The weather data is obtained from NOAA GHCN-Daily dataset. 
    
    - Chill-day model here refers to the original method discussed by *Cesaraccio et al., 2014*.

    - Computation starts from Oct 1st of the previous year and ends on Apr 30th of the target year, using the optimal set of paramters (**M1_gdd_cv_kyoto.r**)

        - *Tc*: Base temperature required to compute chill days.

        - *Rc_thresh*: Accumulated chill day threshold to start accumulating anti-chill days.

3. Train a classification model using LightGBM (*Ke et al., 2017*)

    - The tree-based gradient-boosted classification model is suitable in this case.
        
        - Can model non-linear relationships between the predictors and presponse
        
        - Innately handles possible interactions between the predictors.

        - LightGBM is a fast and efficient implementation of gradient-boosted decision trees.
    
    - Response: is_bloom (1: bloom, 0: no bloom)

    - Predictors: Cd_cumsum, Ca_cumsum, daily_Ca, daily_Cd, tmax, tmin, lat, long, alt, month, day

    - Split the data into 80% training and 20% test set.

        - Training set: All data before the year 2015.

        - Test set: All data including and after the year 2015.

    - Performed an 8-fold cross-validation using the training set to fine-tune the hyperparameters. (**M2_lgb_cv_kyoto.r**)

        - *boosting*: boosting methods.
        - *learning_rate*: learning rate used at each boosting early_stopping_rounds.
        - *max_bin*: maximum number of bins used. Maximum number of bins used for LightGBM. Larger number of bins may cause overfitting.
        - *min_data_in_leaf*: Number of minimum samples in leaves. Smaller numbers may cause overfitting.
        - *max_depth*: Maximum depth of trees. Deeper trees may cause overfitting.
        - *feature_fraction*: Fraction of features used for each split. Higher fractions may cause slower learning speed.
        - *bagging_fraction*: Sample fraction used for every n stage determined by bagging_freq.  May help mitigate overfitting.
        - *bagging_freq*: Sample bagging frequency.
        - *lambda_l2*: Regularization term for L2 regularization.

4. Model evaluation and interpretation

    - Evaluate the model using MAE on the past blooming dates on the test set (year 2015 to 2022)

    - Interprete the importance of the predictors using the feature importance plot.

5. Final prediction for the year 2023

    - Use the final LightGBM model to make the final prediction for the year 2023 in Kyoto.

* Many code lines are commented out to reduce the runtime of the document. Uncomment the lines to run the code.


\newpage

#### 1. Select 10 most closest cities to Kyoto to increase our sample size.
```{r task1, message=FALSE}
# Load libraries and functions.
library(tidyverse)
source("/workspaces/peak-bloom-prediction/code/_shared/F01_functions.r")
```
```{r}
# cherry_sub contains bloom_doy of cities in Japan, Switzerland, and South Korea.
cherry_sub <- read.csv("/workspaces/peak-bloom-prediction/code/_shared/data/A11_cherry_sub.csv") %>%
    distinct(city, bloom_date, .keep_all = TRUE)

# Filter out cities in Japan.
cherry_pca <- cherry_sub %>%
    filter(country == "Japan") %>%
    filter(year == 2021) %>%
    dplyr::select(city, lat, long, alt, bloom_doy) %>%
    distinct(city, .keep_all = TRUE)
rownames(cherry_pca) <- cherry_pca$city
colnames(cherry_pca)[5] <- "2021_bloom_doy"

# Extract the most recent 5 years of bloom_doy for each city in Japan from cherry_sub.
years <- 2017:2020
for (yr in years) {
    temp_data <- cherry_sub %>%
        filter(country == "Japan") %>%
        filter(year == yr) %>%
        dplyr::select(city, bloom_doy) %>%
        distinct(city, .keep_all = TRUE)
    colnames(temp_data)[2] <- paste0(yr, "_bloom_doy")
    cherry_pca <- cherry_pca %>% 
        merge(y = temp_data, by = "city", all.x = TRUE)
}
rownames(cherry_pca) <- cherry_pca$city
cherry_pca <- cherry_pca %>% select(-city) %>% drop_na()
# head(cherry_pca)

# perform pca
pca_result <- prcomp(cherry_pca, scale = TRUE)
pca_out <- data.frame(-1 * pca_result$x)

# Get (Euclidean) distance matrix
kyoto_dist <- data.frame(as.matrix(dist(pca_out))) %>%
    dplyr::select(Kyoto) %>%
    arrange(Kyoto)
# head(kyoto_dist, 11) # Show the top 11 cities (including Kyoto itself) with the smallest distance to Kyoto.

# Get city names
kyoto_group <- rownames(kyoto_dist)[1:11]
```

The top 10 closest cities to Kyoto in terms of their geographical locations (lat, long, alt) and 5 most recent bloom_doy are: `r kyoto_group[2:11]`

\newpage

#### 2. Compute the accumulated chill days and anti-chill days (Cd_cumsum, Ca_cumsum)
```{r task2, warning=FALSE, message=FALSE, eval=FALSE}
library(rnoaa)

# Pull weather stations ids for the stations in Japan.
weather_stations <- ghcnd_stations() %>%
    filter(last_year > 2021) %>%
    filter(first_year < 1954) %>%
    distinct(id, .keep_all = TRUE) %>%
    filter(str_sub(id, 1, 2) %in% c("JA")) %>%
    filter(name %in% toupper(kyoto_group))
# write.csv(weather_stations, "./code/kyoto/data/A11_weather_stations_kyoto.csv", row.names = FALSE)
# weather_stations <- read.csv("./code/kyoto/data/A11_weather_stations_kyoto.csv")

# Get the weather station ids for the target cities.
city_station_pair <- weather_stations %>% 
    mutate(city = str_to_title(name)) %>%
    select(-c(name, state, gsn_flag, wmo_id, element, first_year, last_year)) %>%
    rename_with(~"lat", latitude) %>%
    rename_with(~"long", longitude) %>%
    rename_with(~"alt", elevation)
# write.csv(city_station_pair, "./code/kyoto/data/A11_city_station_pairs.csv", row.names = FALSE)
# city_station_pair <- read.csv("/workspaces/peak-bloom-prediction/code/kyoto/data/A11_city_station_pairs.csv")

# Get temperature data for the target cities.
# - get_imp_temperature() is a function downloads the temperature data, and impute any missing values using the predictive mean matching method.
kyoto_weather <- F01_get_imp_temperature(
    city_station_pair = city_station_pair
    )
# write.csv(kyoto_weather, "./code/kyoto/data/A12_kyoto_temperature.csv", row.names = FALSE)
# kyoto_weather <- read.csv("/workspaces/peak-bloom-prediction/code/kyoto/data/A12_kyoto_temperature.csv")

# Find optimal Rc_thresh and Tc using the chill-day model
# - CAUTION: running the code below may require a high computational power.
source("/workspaces/peak-bloom-prediction/code/kyoto/M1_gdd_cv_kyoto.r")
# best_gdd_params <- read.csv("/workspaces/peak-bloom-prediction/code/kyoto/data/M12_Kyoto_gdd_best.csv")[1, ]

# Compute daily_Ca, daily_Cd, Ca_cumsum(=AGDD), Cd_cumsum 
# - compute_gdd computes daily_Cd, daily_Ca, Cd_cumsum, and Ca_cumsum based on Cesaraccio et al., 2014
kyoto_gdd <- F01_compute_gdd(
    weather_df = kyoto_weather
    , noaa_station_ids = unique(kyoto_weather$id)
    , Rc_thresh = best_gdd_params[["Rc_thresholds"]]
    , Tc = best_gdd_params[["Tcs"]])

# Merge the data with city names and their lat, long, alt
kyoto_group <- city_station_pair$city
cherry_city_blooms <- cherry_sub %>%
    filter(city %in% kyoto_group) %>%
    select(city, bloom_doy, bloom_date)

# We use kyoto_gdd2 for the final model fitting and prediction
kyoto_gdd2 <- kyoto_gdd %>%
    merge(y = city_station_pair, by = "id", all.x = TRUE) %>%
    merge(y = cherry_city_blooms
    , by.x = c("city", "date")
    , by.y = c("city", "bloom_date"), all.x = TRUE) %>%
    mutate(doy = as.integer(strftime(date, format = "%j"))) %>%
    mutate(is_bloom = ifelse(!is.na(bloom_doy), 1, 0))
# write.csv(kyoto_gdd2, "/workspaces/peak-bloom-prediction/code/kyoto/data/A14_kyoto_gdd.csv", row.names = FALSE)
# kyoto_gdd2 <- read.csv("/workspaces/peak-bloom-prediction/code/kyoto/data/A14_kyoto_gdd.csv")
# head(kyoto_gdd2)
```

```{r task22, message=FALSE, warning=FALSE}

best_gdd_params <- read.csv("/workspaces/peak-bloom-prediction/code/kyoto/data/M12_Kyoto_gdd_best.csv")[1, ]
kyoto_gdd2 <- read.csv("/workspaces/peak-bloom-prediction/code/kyoto/data/A14_kyoto_gdd.csv")

# Check the distribution of Ca_cumsum of those is_bloom = 1
# hist(kyoto_gdd2 %>% filter(is_bloom == 1) %>% pull(Ca_cumsum), breaks = 30)  # for all target cities
# hist(kyoto_gdd2 %>% filter(is_bloom == 1) %>% filter(city =="Kyoto") %>% pull(Ca_cumsum), breaks =30) # for Kyoto

# Check Ca_cumsum and bloom_doy trends in Kyoto.
kyoto_blooms <- kyoto_gdd2 %>% filter(city == "Kyoto") %>%filter(is_bloom == 1)
# plot(kyoto_blooms$year, kyoto_blooms$Ca_cumsum, type = "l")
# plot(kyoto_blooms$year, kyoto_blooms$bloom_doy, type = "l")
# plot(kyoto_blooms$Cd_cumsum, kyoto_blooms$bloom_doy, type = "p")
ggplot(data = kyoto_blooms, aes(x = Ca_cumsum, y = bloom_doy)) +
    geom_point(size = 2)+
    theme_bw() + 
    ylim(c(70, 130))
```
Here we can see that Ca_cumsum alone may not be sufficient to explain the bloom_doy trend as they are not in a completely linear relationship.

\newpage

#### 3. Train a classification model using LightGBM
```{r task3}
# Perform 8-fold cross-validation to find the best set of parameters.
# * CAUTION: running the code below may require a high computational power.
# source(./code/kyoto/M2_lgb_cv_kyoto.r)
```

#### 4. Model evaluation and interpretation

##### 4.1 Final model training
```{r task41, warning=FALSE, message=FALSE}
library(lightgbm)

# Load the best parameter set
best_params <- read.csv("/workspaces/peak-bloom-prediction/code/kyoto/data/M23_lgb_best_score_kyoto3.csv")
# best_params

# Load data
cherry_gdd <- read.csv("/workspaces/peak-bloom-prediction/code/kyoto/data/A14_kyoto_gdd.csv") %>%
    filter(month %in% c(3, 4))  

feature_names <- c("tmax", "tmin", "daily_Ca", "daily_Cd", "Cd_cumsum", "Ca_cumsum", "lat", "long", "alt", "month", "day")
target_col <- "is_bloom"

# Make prediction on the last 10 years
target_years <- 2015:2022

train_set <- cherry_gdd %>% filter(!(year %in% target_years))

# Perform stratified under-sampling from train_set to balance the number of is_bloom = 1 and 0
set.seed(42)
train_isbloom <- train_set %>% filter(is_bloom == 1)
train_nobloom <- train_set %>% filter(is_bloom == 0)
train_sample <- train_nobloom[sample(nrow(train_nobloom), nrow(train_isbloom) *1.5), ] %>% bind_rows(train_isbloom)

test_set <- cherry_gdd %>%filter(year %in% target_years)
test_isbloom <- test_set %>% filter(is_bloom == 1)
test_nobloom <- test_set %>% filter(is_bloom == 0)
test_sample <- test_nobloom[sample(nrow(test_nobloom), nrow(test_isbloom) *1.5), ] %>% bind_rows(test_isbloom)
# table(test_sample$is_bloom)

# Train the final model using the best parameter set and the training set.
dtrain <- lgb.Dataset(
    data = data.matrix(train_sample[, feature_names])
    , label = train_sample[, target_col]
    , params = list(
        max_bin = best_params$max_bins
    )
)

dtest <- lgb.Dataset(
    data = data.matrix(test_set[, feature_names])
    , label = test_set[, target_col]
)

valids <- list(test = dtest)

n_boosting_rounds <- 1000

params <- list(
    objective = "binary"
    , metric = c("AUC")  
    , is_enable_sparse = TRUE
    , boosting = as.character(best_params[["boostings"]])
    , learning_rate = as.numeric(best_params[["learning_rates"]])
    , min_data_in_leaf = as.numeric(best_params[["min_data_in_leaf"]])
    , max_depth = as.numeric(best_params[["max_depth"]])
    , feature_fraction = as.numeric(best_params[["feature_fractions"]])
    , bagging_fraction = as.numeric(best_params[["bagging_fractions"]])
    , bagging_freq = as.numeric(best_params[["bagging_freqs"]])
    , lambda_l2 = as.numeric(best_params[["lambda_l2s"]])
    , early_stopping_rounds = as.integer(n_boosting_rounds * 0.1)
    , seed = 42L
)

lgb_final <- lgb.train(
    data = dtrain
    , params = params
    , valids = valids
)

best_score <- lgb_final$best_score
```

The final model's test AUC is `r {best_score}`

##### 4.2 Model evaluation

```{r task42, message=FALSE, warning=FALSE, eval=FALSE}
library(tidyverse)
# Here we try different values of p_thresh and choose the one that returns the best MAE over the last 8 years.
MAE_p <- c()
MAE_table <- data.frame(
    year = 2015:2022
    , actual_bloom_date = NA
    , predicted_bloom_date = NA
    , diff = 0
)

actual_bloom_dates <- read.csv("/workspaces/peak-bloom-prediction/competition_rules/data/kyoto.csv")
kyoto_gdd <- read.csv("/workspaces/peak-bloom-prediction/code/kyoto/data/A14_kyoto_gdd.csv")

target_p_thresh <- seq(0.4, 0.7, by = 0.05)
current_MAE_p <- 999
for (p_thresh in target_p_thresh) {
    
    for (yr in MAE_table$year) {
        # p_thresh = 0.4        
        actual_bloom_date <- actual_bloom_dates %>%filter(year == yr) %>% pull(bloom_date)
        mae_set <- kyoto_gdd %>% filter(year == yr)

        mae_pred <- predict(lgb_final, data.matrix(mae_set[, feature_names]))
        mae_set$pred_prob <- mae_pred
        mae_set$pred_bin <- ifelse(mae_pred > p_thresh, 1, 0)

        # Prediction based on diff probability thresholds
        predicted_bloom_date_p_thresh_idx <- which(mae_set$pred_prob > p_thresh)[1]
        predicted_bloom_date_p_thresh <- mae_set[predicted_bloom_date_p_thresh_idx, "date"]
        
        diff <- as.numeric(as.Date(actual_bloom_date, format = "%Y-%m-%d"))- as.numeric(as.Date(predicted_bloom_date_p_thresh, format = "%Y-%m-%d"))
        
        MAE_table[MAE_table$year ==yr, ]<- c(yr
        , actual_bloom_date, predicted_bloom_date_p_thresh
        , as.numeric(diff))

    }
    yr_MAE <- mean(abs(as.numeric(MAE_table$diff)))
    MAE_p <- c(MAE_p, yr_MAE, na.rm = TRUE)
    
    if (yr_MAE < current_MAE_p) {
        best_MAE_table <- MAE_table
        current_MAE_p <- yr_MAE
    }
}
# write.csv(data.frame(MAE_p = MAE_p, target_p_thresh = target_p_thresh), file = "/workspaces/peak-bloom-prediction/code/kyoto/data/A15_kyoto_MAE_p.csv", row.names = FALSE)
```

```{r}
ggplot(data = best_MAE_table, aes(x = year, y = diff)) + geom_point(size = 2)
```

```{r task422, echo=FALSE}
MAE_p_df <- read.csv("/workspaces/peak-bloom-prediction/code/kyoto/data/A15_kyoto_MAE_p.csv")
target_p_thresh <- MAE_p_df[["target_p_thresh"]]
best_p_thresh <- target_p_thresh[which.min(MAE_p_df[["MAE_p"]])]
MAE_p <- MAE_p_df[["MAE_p"]]
```
* It seems that the best p_thresh is `r {best_p_thresh}`, which gives the lowest MAE of `r {min(MAE_p)}`.
* Most recent years show MAE above 0. Therefore, we subtract ceilding(MAE/2)=`r ceiling(min(MAE_p)/2)` from the final predicted doy later on.

```{r task423, echo=FALSE}

##### 4.3 Model interpretation
```{r task43, message=FALSE, warning=FALSE}
# Interpret the model using the feature importance plot
lgb_imp <- lgb.importance(lgb_final)
lgb.plot.importance(lgb_imp, top_n = 5L, measure = "Gain")
```

\newpage

#### 5. Final prediction for the year 2023
```{r task5, message=FALSE, warning=FALSE, eval=FALSE}
# Weather data for Oct 2022 to Feb 2023 downloaded from NOAA, 2023 March and April obtained from AccuWeather
# https://www.accuweather.com/en/jp/kyoto-shi/224436/weather-forecast/224436

city_station_pair <- read.csv("/workspaces/peak-bloom-prediction/code/kyoto/data/A11_city_station_pairs.csv") %>% filter(city == "Kyoto")

temp_2223 <- F01_get_imp_temperature(
    city_station_pair = city_station_pair
    , date_min = "2022-10-01", date_max = "2023-04-30") %>% 
    mutate(year = as.integer(strftime(date, format = "%Y"))) %>%
    filter(year %in% c(2022, 2023)) %>%
    select(id, date, year, month, day, tmin, tmax) %>% "rownames<-"(NULL)

data_2023 <- read.csv("/workspaces/peak-bloom-prediction/code/_shared/data/city_weather_2023.csv") %>%
    filter(city == "Kyoto") %>%
    mutate(year = 2023) %>%
    mutate(month = as.integer(strftime(date, "%m"))) %>%
    mutate(day = as.integer(strftime(date, "%d"))) %>%
    select(id, date, year, month, day, tmin, tmax)

merged_2223 <- rbind(temp_2223, data_2023) %>% "rownames<-"(NULL)
# write.csv(merged_2223, "./code/kyoto/data/A16_kyoto_weather_2023.csv", row.names = FALSE)
```

```{r}
city_station_pair <- read.csv("/workspaces/peak-bloom-prediction/code/kyoto/data/A11_city_station_pairs.csv") %>% filter(city == "Kyoto")
merged_2223 <- read.csv("/workspaces/peak-bloom-prediction/code/kyoto/data/A16_kyoto_weather_2023.csv")

# Compute the chill days and anti-chill days
best_gdd_params <- read.csv("/workspaces/peak-bloom-prediction/code/kyoto/data/M12_Kyoto_gdd_best.csv")[1, ]
gdd_2223 <- F01_compute_gdd(merged_2223
        , noaa_station_ids = unique(merged_2223$id)
        , Rc_thresh = best_gdd_params[["Rc_thresholds"]]
        , Tc = best_gdd_params[["Tcs"]]) %>%
    mutate(doy = as.integer(strftime(date, "%j"))) %>% 
    merge(y = city_station_pair, by = "id"
        , all.x = TRUE) %>% "rownames<-"(NULL) %>%
    filter(month %in% c(3, 4))
# write.csv(gdd_2223, "./code/kyoto/data/A17_kyoto_gdd_2023.csv", row.names = FALSE)
gdd_2223 <- read.csv("/workspaces/peak-bloom-prediction/code/kyoto/data/A17_kyoto_gdd_2023.csv")

# Make final prediction for 2023 Kyoto
final_pred <- predict(lgb_final, as.matrix(gdd_2223[, feature_names]))
final_pred_date <- gdd_2223[which(final_pred > best_p_thresh)[1], "date"]
# Compute doy
final_doy <- as.integer(strftime(final_pred_date, "%j"))

```

* The final predicted cherry blossom date for Kyoto 2023 is `r {final_pred_date}`, which corresponds to doy = `r {final_doy}`.
* However, since the MAE from above is `r {min(MAE_p)}`, we subtract `r {ceiling(min(MAE_p))}` to the doy to get the final predicted doy, which is `r {final_doy - ceiling(min(MAE_p))}`.
* Since this model looks weak, we train a non-temperature-based model and average the final results from the two models to get the final prediction.

```{r echo=FALSE, message=FALSE, warning=FALSE}
final_pred_df <- data.frame(city = "Kyoto", method = "ML", bloom_doy = final_doy - ceiling(min(MAE_p)))
final_pred_probs <- data.frame(city = "Kyoto", date = gdd2223[, "date"], pred_probs = final_pred)

# write.csv(final_pred_df, "/workspaces/peak-bloom-prediction/code/kyoto/data/A19_final_lgb_predDay_kyoto.csv", row.names = FALSE)
# write.csv(final_pred_probs, "/workspaces/peak-bloom-prediction/code/kyoto/data/A19_final_predProbs_kyoto.csv", row.names = FALSE)
```