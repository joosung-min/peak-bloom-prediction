---
title: "Peak Cherry Blossom Prediction in Kyoto 2023"
author: "Joosung (Sonny) Min"
output: html_document
---

In this document, we perform LightGBM using the weather data from the closest cities to Kyoto, Japan. 

1. Select 10 most closest cities to Kyoto to increase our sample size.

    - We use 5 most recent bloom_doy along with the geographical location (lat, long, alt) of all cities listed in Japan on data/japan.csv.

    - Perform PCA and compute the Euclidean distances between the cities, and select top 10 cities with the smallest distance to Kyoto.

2. Compute the accumulated chill days and anti-chill days (Cd_cumsum, Ca_cumsum) 
    
    - The weather data is obtained from NOAA GHCN-Daily dataset. 
    
    - Chill-day model here refers to the original method discussed by *Cesaraccio et al., 2014*.

    - Computation starts from Oct 1st of the previous year and ends on Apr 30th of the target year, using the optimal set of paramters (**M1_gdd_cv_kyoto.r**)

        - *Tc*: Base temperature required to compute chill days.

        - *Rc_thresh*: Accumulated chill day threshold to start accumulating anti-chill days.

3. Train a classification model using LightGBM (*Ke et al., 2017*)

    - The tree-based gradient-boosted classification model is suitable in this case.
        
        - Can model non-linear relationships between the predictors and presponse
        
        - Innately handles possible interactions between the predictors.

        - LightGBM is a fast and efficient implementation of gradient-boosted decision trees.
    
    - Response: is_bloom (1: bloom, 0: no bloom)

    - Predictors: Cd_cumsum, Ca_cumsum, daily_Ca, daily_Cd, tmax, tmin, lat, long, alt, month, day

    - Split the data into 80% training and 20% test set.

        - Training set: All data before the year 2015.

        - Test set: All data including and after the year 2015.

    - Performed an 8-fold cross-validation using the training set to fine-tune the hyperparameters. (**M2_lgb_cv_kyoto.r**)

        - *boosting*: boosting methods.
        - *learning_rate*: learning rate used at each boosting early_stopping_rounds.
        - *max_bin*: maximum number of bins used. Maximum number of bins used for LightGBM. Larger number of bins may cause overfitting.
        - *min_data_in_leaf*: Number of minimum samples in leaves. Smaller numbers may cause overfitting.
        - *max_depth*: Maximum depth of trees. Deeper trees may cause overfitting.
        - *feature_fraction*: Fraction of features used for each split. Higher fractions may cause slower learning speed.
        - *bagging_fraction*: Sample fraction used for every n stage determined by bagging_freq.  May help mitigate overfitting.
        - *bagging_freq*: Sample bagging frequency.
        - *lambda_l2*: Regularization term for L2 regularization.

4. Model evaluation and interpretation

    - Evaluate the model using MAE on the past blooming dates on the test set (year 2015 to 2022)

    - Interprete the importance of the predictors using the feature importance plot.

5. Final prediction for the year 2023

    - Use the final LightGBM model to make the final prediction for the year 2023 in Kyoto.

\newpage

#### 1. Select 10 most closest cities to Kyoto to increase our sample size.
```{r}
# Load libraries and functions.
library(tidyverse)
source("/workspaces/peak-bloom-prediction/code/_shared/F01_functions.r")

# cherry_sub contains bloom_doy of cities in Japan, Switzerland, and South Korea.
cherry_sub <- read.csv("/workspaces/peak-bloom-prediction/code/_shared/data/A11_cherry_sub.csv") %>%
    distinct(city, bloom_date, .keep_all = TRUE)

# Filter out cities in Japan.
cherry_pca <- cherry_sub %>%
    filter(country == "Japan") %>%
    filter(year == 2021) %>%
    dplyr::select(city, lat, long, alt, bloom_doy) %>%
    distinct(city, .keep_all = TRUE)
rownames(cherry_pca) <- cherry_pca$city
colnames(cherry_pca)[5] <- "2021_bloom_doy"

# Extract the most recent 5 years of bloom_doy for each city in Japan from cherry_sub.
years <- 2017:2020
for (yr in years) {
    temp_data <- cherry_sub %>%
        filter(country == "Japan") %>%
        filter(year == yr) %>%
        dplyr::select(city, bloom_doy) %>%
        distinct(city, .keep_all = TRUE)
    colnames(temp_data)[2] <- paste0(yr, "_bloom_doy")
    cherry_pca <- cherry_pca %>% 
        merge(y = temp_data, by = "city", all.x = TRUE)
}
rownames(cherry_pca) <- cherry_pca$city
cherry_pca <- cherry_pca %>% select(-city) %>% drop_na()
# head(cherry_pca)

# perform pca
pca_result <- prcomp(cherry_pca, scale = TRUE)
pca_out <- data.frame(-1 * pca_result$x)

# Get (Euclidean) distance matrix
kyoto_dist <- data.frame(as.matrix(dist(pca_out))) %>%
    dplyr::select(Kyoto) %>%
    arrange(Kyoto)
# head(kyoto_dist, 11) # Show the top 11 cities (including Kyoto itself) with the smallest distance to Kyoto.

# Get city names
kyoto_group <- rownames(kyoto_dist)[1:11]
```

The top 10 closest cities to Kyoto in terms of their geographical locations (lat, long, alt) and 5 most recent bloom_doy are: `r kyoto_group[2:11]`

\newpage

#### 2. Compute the accumulated chill days and anti-chill days (Cd_cumsum, Ca_cumsum)
```{r}
library(rnoaa)

# Pull weather stations ids for the stations in Japan.
weather_stations <- ghcnd_stations() %>%
    filter(last_year > 2021) %>%
    filter(first_year < 1954) %>%
    distinct(id, .keep_all = TRUE) %>%
    filter(str_sub(id, 1, 2) %in% c("JA")) %>%
    filter(name %in% toupper(kyoto_group))
# write.csv(weather_stations, "./code/kyoto/data/A11_weather_stations_kyoto.csv", row.names = FALSE)
# weather_stations <- read.csv("./code/kyoto/data/A11_weather_stations_kyoto.csv")

# Get the weather station ids for the target cities.
city_station_pair <- weather_stations %>% 
    mutate(city = str_to_title(name)) %>%
    select(-c(name, state, gsn_flag, wmo_id, element, first_year, last_year)) %>%
    rename_with(~"lat", latitude) %>%
    rename_with(~"long", longitude) %>%
    rename_with(~"alt", elevation)
# # write.csv(city_station_pair, "./code/kyoto/data/A11_city_station_pairs.csv", row.names = FALSE)

# Get temperature data for the target cities.
# - get_imp_temperature() is a function downloads the temperature data, and impute any missing values using the predictive mean matching method.
kyoto_weather <- F01_get_imp_temperature(
    city_station_pair = city_station_pair
    )
# # write.csv(kyoto_weather, "./code/kyoto/data/A12_kyoto_temperature.csv", row.names = FALSE)
# kyoto_weather <- read.csv("./code/kyoto/data/A12_kyoto_temperature.csv")

# Find optimal Rc_thresh and Tc using the chill-day model
# - CAUTION: running the code below may require a high computational power.
# source("./code/kyoto/M1_gdd_cv_kyoto.r")
best_gdd_params <- read.csv("/workspaces/peak-bloom-prediction/code/kyoto/data/M12_Kyoto_gdd_best.csv")[1, ]

# Compute daily_Ca, daily_Cd, Ca_cumsum(=AGDD), Cd_cumsum 
# - compute_gdd computes daily_Cd, daily_Ca, Cd_cumsum, and Ca_cumsum based on Cesaraccio et al., 2014
kyoto_gdd <- F01_compute_gdd(
    weather_df = kyoto_weather
    , noaa_station_ids = unique(kyoto_weather$id)
    , Rc_thresh = best_gdd_params[["Rc_thresholds"]]
    , Tc = best_gdd_params[["Tcs"]])

# Merge the data with city names and their lat, long, alt
city_station_pair <- read.csv("/workspaces/peak-bloom-prediction/code/kyoto/data/A11_city_station_pairs.csv")
kyoto_group <- city_station_pair$city
cherry_city_blooms <- cherry_sub %>%
    filter(city %in% kyoto_group) %>%
    select(city, bloom_doy, bloom_date)

# We use kyoto_gdd2 for the final model fitting and prediction
kyoto_gdd2 <- kyoto_gdd %>%
    merge(y = city_station_pair, by = "id", all.x = TRUE) %>%
    merge(y = cherry_city_blooms
    , by.x = c("city", "date")
    , by.y = c("city", "bloom_date"), all.x = TRUE) %>%
    mutate(doy = as.integer(strftime(date, format = "%j"))) %>%
    mutate(is_bloom = ifelse(!is.na(bloom_doy), 1, 0))
write.csv(kyoto_gdd2, "/workspaces/peak-bloom-prediction/code/kyoto/data/A14_kyoto_temp_gdd.csv", row.names = FALSE)
# kyoto_gdd2 <- read.csv("./code/kyoto/data/A14_kyoto_temp_gdd.csv")
head(kyoto_gdd2)
```

```{r}
# Check the distribution of Ca_cumsum of those is_bloom = 1
# hist(kyoto_gdd2 %>% filter(is_bloom == 1) %>% pull(Ca_cumsum), breaks = 30)  # for all target cities
# hist(kyoto_gdd2 %>% filter(is_bloom == 1) %>% filter(city =="Kyoto") %>% pull(Ca_cumsum), breaks =30) # for Kyoto

# Check Ca_cumsum and bloom_doy trends in Kyoto.
kyoto_blooms <- kyoto_gdd2 %>% filter(city == "Kyoto") %>%filter(is_bloom == 1)
# plot(kyoto_blooms$year, kyoto_blooms$Ca_cumsum, type = "l")
# plot(kyoto_blooms$year, kyoto_blooms$bloom_doy, type = "l")
# plot(kyoto_blooms$Cd_cumsum, kyoto_blooms$bloom_doy, type = "p")
ggplot(data = kyoto_blooms, aes(x = Ca_cumsum, y = bloom_doy)) +
    geom_point(size = 2)+
    theme_bw() + 
    ylim(c(70, 130))
```
Here we can see that Ca_cumsum alone may not be sufficient to explain the bloom_doy trend as they are not in a completely linear relationship.

\newpage

#### 3. Train a classification model using LightGBM
```{r}
# Perform 8-fold cross-validation to find the best set of parameters.
# * CAUTION: running the code below may require a high computational power.
# source(./code/kyoto/M2_lgb_cv_kyoto.r)
```

#### 4. Model evaluation and interpretation
```{r}
library(lightgbm)

# Load the best parameter set
best_params <- read.csv("./code/kyoto/data/M23_lgb_best_score_kyoto3.csv")
best_params
# best_params <- read.csv("./code/kyoto/data/archive/best1/M23_lgb_best_score_kyoto3.csv")
# best_params

# # contains actual bloom dates + past gdd info
# cherry_gdd <- read.csv("./code/kyoto/data/A14_kyoto_temp_gdd.csv") %>%
#     filter(month %in% c(3, 4))  

# feature_names <- c("tmax", "tmin", "daily_Ca", "daily_Cd", "Cd_cumsum", "Ca_cumsum", "lat", "long", "alt", "month", "day")
# # feature_names <- c("tmax", "tmin", "daily_Ca", "daily_Cd", "Cd_cumsum", "Ca_cumsum", "lat", "long", "alt", "doy")
# target_col <- "is_bloom"

# # Make prediction on the last 4 years
# target_years <- 2013:2022

# train_set <- cherry_gdd %>% filter(!(year %in% target_years))

# # stratified under-sampling from train_set to balance the number of is_bloom = 1 and 0
# train_isbloom <- train_set %>% filter(is_bloom == 1)
# train_nobloom <- train_set %>% filter(is_bloom == 0)
# train_sample <- train_nobloom[sample(nrow(train_nobloom), nrow(train_isbloom) *1.5), ] %>% bind_rows(train_isbloom)

# test_set <- cherry_gdd %>%filter(year %in% target_years)
# test_isbloom <- test_set %>% filter(is_bloom == 1)
# test_nobloom <- test_set %>% filter(is_bloom == 0)
# test_sample <- test_nobloom[sample(nrow(test_nobloom), nrow(test_isbloom) *1.5), ] %>% bind_rows(test_isbloom)
# table(test_sample$is_bloom)

# dtrain <- lgb.Dataset(
#     data = data.matrix(train_sample[, feature_names])
#     , label = train_sample[, target_col]
#     , params = list(
#         max_bin = best_params$max_bins
#     )
# )

# dtest <- lgb.Dataset(
#     data = data.matrix(test_set[, feature_names])
#     , label = test_set[, target_col]
# )

# valids <- list(test = dtest)

# n_boosting_rounds <- 1000

# params <- list(
#     objective = "binary"
#     , metric = c("binary_logloss")
#     , is_enable_sparse = TRUE
#     , boosting = as.character(best_params[["boostings"]])
#     , learning_rate = as.numeric(best_params[["learning_rates"]])
#     , min_data_in_leaf = as.numeric(best_params[["min_data_in_leaf"]])
#     , max_depth = as.numeric(best_params[["max_depth"]])
#     , feature_fraction = as.numeric(best_params[["feature_fractions"]])
#     , bagging_fraction = as.numeric(best_params[["bagging_fractions"]])
#     , bagging_freq = as.numeric(best_params[["bagging_freqs"]])
#     , lambda_l2 = as.numeric(best_params[["lambda_l2s"]])
#     , early_stopping_rounds = as.integer(n_boosting_rounds * 0.1)
#     , seed = 42L
# )

# lgb_final <- lgb.train(
#     data = dtrain
#     , params = params
#     , valids = valids
# )

# pred <- predict(lgb_final, as.matrix(test_set[, feature_names]))
# hist(pred, breaks =100)
# test_set$predicted <- ifelse(pred > 0.5, 1, 0)
# tail(sort(pred))

# # Confusion matrix
# library(caret)
# confusionMatrix(factor(test_set$predicted), factor(test_set$is_bloom))

# # ROC curve
# library(ROCR)
# roc_pred <- prediction(pred, test_set$is_bloom)
# roc <- performance(roc_pred, "sens", "spec")
# plot(roc, main="ROC curve")
# abline(a=0, b=1)

# # Feature importance
# lgb_imp <- lgb.importance(lgb_final)
# lgb_imp
# lgb.plot.importance(lgb_imp, top_n = 10L, measure = "Gain")

# # Compute the MAE for the most recent years
# mae <- F01_compute_MAE(target_city = "Kyoto", cherry_gdd = cherry_gdd, lgb_final = lgb_final, target_years = c(2012:2022), p_thresh = 0.5, peak = TRUE)
# mae

# # Generate and save the prediction plot for the most recent years
# F01_pred_plot_past(target_city = "Kyoto", cherry_gdd = cherry_gdd, lgb_final = lgb_final, target_years = c(2019:2022), p_thresh = 0.5, peak = TRUE)


# #######################################
# # Final prediction for 2023
# #######################################

# # Weather data for 2023 march and april obtained from 

# # city_station_pair <- read.csv("./code/kyoto/data/A11_city_station_pairs.csv") %>% filter(city == "Kyoto")

# # temp_2223 <- F01_get_imp_temperature(
# #     city_station_pair = city_station_pair
# #     , target_country = c("Japan")
# #     , date_min = "2022-10-01", date_max = "2023-04-30") %>% 
# #     mutate(year = as.integer(strftime(date, format = "%Y"))) %>%
# #     filter(year %in% c(2022, 2023)) %>%
# #     select(id, date, year, month, day, tmin, tmax) %>% "rownames<-"(NULL)
# # head(temp_2223)
# # tail(temp_2223)

# # data_2023 <- read.csv("./code/_shared/data/city_weather_2023.csv") %>%
# #     filter(city == "Kyoto") %>%
# #     mutate(year = 2023) %>%
# #     mutate(month = as.integer(strftime(date, "%m"))) %>%
# #     mutate(day = as.integer(strftime(date, "%d"))) %>%
# #     select(id, date, year, month, day, tmin, tmax)

# # merged_2223 <- rbind(temp_2223, data_2023) %>% "rownames<-"(NULL)
# # tail(merged_2223)
# # write.csv(merged_2223, "./code/kyoto/data/A16_kyoto_weather_2023.csv", row.names = FALSE)
# merged_2223 <- read.csv("./code/kyoto/data/A16_kyoto_weather_2023.csv")

# # Compute GDD
# # best_gdd_params <- read.csv("./code/kyoto/data/M12_Kyoto_gdd_best.csv")[1, ]
# # best_gdd_params
# # gdd_2223 <- F01_compute_gdd(merged_2223
# #     , noaa_station_ids = unique(merged_2223$id)
# #     ,Rc_thresh = best_gdd_params[["Rc_thresholds"]]
# #     , Tc = best_gdd_params[["Tcs"]]) %>%
# #     mutate(doy = as.integer(strftime(date, "%j"))) %>% 
# #     merge(y = city_station_pair, by = "id"
# #     , all.x = TRUE) %>% "rownames<-"(NULL) %>%
# #     filter(month %in% c(3, 4))
# # dim(gdd_2223)    
# # head(gdd_2223)
# # tail(gdd_2223)
# # write.csv(gdd_2223, "./code/kyoto/data/A17_kyoto_gdd_2023.csv", row.names = FALSE)
# gdd_2223 <- read.csv("./code/kyoto/data/A17_kyoto_gdd_2023.csv")

# # Make final prediction for 2023 Kyoto
# final_pred <- predict(lgb_final, as.matrix(gdd_2223[, feature_names]))

# p_final_pred <- F01_pred_plot_final(target_city = "Kyoto"
#     , year_data = gdd_2223
#     , feature_names = feature_names
#     , lgb_final = lgb_final
#     , p_thresh = 0.5
#     , peak = FALSE)
# p_final_pred

# ggsave("./code/kyoto/outputs/kyoto_2023_prediction_plot.png", p_final_pred
#     , width = 10, height = 6, units = "in", dpi = 80)
```