{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit lightGBM: Washington D.C. and New York City\n",
    "\n",
    "* Response: is_bloom \n",
    "\n",
    "* Predictors: \n",
    "    * Ca_cumsum: Cumulative chill day-based growing degree days. \n",
    "    * Cd_cumsum: Cumulative chill day-based anti-growing degree days. \n",
    "    * prcp_cumsum: Cumulated precipitation.\n",
    "    * AGDD: Accumulated growing degree days, as instructed in the usa-npn datafield description file.\n",
    "    * lat: latitude\n",
    "    * long: longitude\n",
    "    * alt: altitude\n",
    "    * month, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load necessary packages\n",
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import yaml\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics\n",
    "import pickle\n",
    "\n",
    "import optuna\n",
    "\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "# import s3fs\n",
    "from datetime import datetime\n",
    "\n",
    "with open(\"./_config.yaml\", \"r\") as file:\n",
    "    cherry_config = yaml.safe_load(file)\n",
    "comp_data_dir = cherry_config['competition_data']\n",
    "data_dir = cherry_config['data_dir']     # data generated from A__dataPrep.ipynb\n",
    "model_dir = cherry_config['model_dir']   # output dir for the best trained lgb models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for computing Ca_cumsum and Cd_cumsum\n",
    "\n",
    "def compute_gdd(r):\n",
    "    gdd = (r[['tmax']].item() + r[['tmin']].item())/2\n",
    "    return gdd if gdd > 0 else 0\n",
    "\n",
    "\n",
    "def chill_days(r, Tc):\n",
    "    '''\n",
    "    Following Cesaraccio\n",
    "    '''\n",
    "    Tmin = r[['tmin']].item()\n",
    "    Tmax = r[['tmax']].item()\n",
    "    Tmean = (Tmin + Tmax)/2\n",
    "\n",
    "    if (0 <= Tc) & (Tc <= Tmin) & (Tmin <= Tmax):\n",
    "        Cd = 0\n",
    "        Ca = Tmean - Tc\n",
    "\n",
    "    elif (0 <= Tmin) & (Tmin <= Tc) & (Tc <= Tmax):\n",
    "        Cd = -1 * ((Tmean - Tmin) - ((Tmax - Tc)/2))\n",
    "        Ca = (Tmax - Tc)/2\n",
    "    \n",
    "    elif (0 <= Tmin) & (Tmin <= Tmax) & (Tmax <= Tc):\n",
    "        Cd = -1 * (Tmean - Tmin)\n",
    "        Ca = 0\n",
    "    \n",
    "    elif (Tmin <= 0) & (0 <= Tmax) & (Tmax <= Tc):\n",
    "        Cd = -1 * (Tmax / (Tmax - Tmin)) * (Tmax/2)\n",
    "        Ca = 0\n",
    "    \n",
    "    elif (Tmin <= 0) & (0 <= Tc) & (Tc <= Tmax):\n",
    "        Cd = -1 * ((Tmax / (Tmax - Tmin)) * (Tmax/2) - ((Tmax - Tc)/2))\n",
    "        Ca = (Tmax - Tc) / 2\n",
    "    \n",
    "    elif (Tmax < 0):\n",
    "        Cd = 0\n",
    "        Ca = 0\n",
    "    \n",
    "    else:\n",
    "        Cd = 0\n",
    "        Ca = 0\n",
    "\n",
    "    # r['Cd'] = Cd\n",
    "    # r['Ca'] = Ca\n",
    "\n",
    "    return Cd, Ca\n",
    "\n",
    "def compute_cgdd(station_df, station_id, Rc_thresh, Tc):\n",
    "\n",
    "    # Computes daily_Ca, daily_Cd, Ca_cumsum, Cd_cumsum.\n",
    "    # weather_df should have at least: tmax, tmin\n",
    "    \n",
    "    # Rc_thresh and Tc are learnt from gdd_model\n",
    "    # Rc_thresh accumulated Cd threshold to start accumulating GDD.\n",
    "    # Tc: Threshold temperature for computing Ca and Cd.\n",
    "\n",
    "    output_list = {}\n",
    "    \n",
    "    Ca_Cd_df = station_df.copy()\n",
    "    \n",
    "    Ca_Cd_df['date'] = Ca_Cd_df.apply(lambda x : \"-\".join([str(x[\"year\"]), str(x[\"month\"]), str(x[\"day\"])]), axis = 1)\n",
    "    Ca_Cd_df['date'] = pd.to_datetime(Ca_Cd_df['date'])\n",
    "    \n",
    "    Ca_Cd_df['daily_Cd'], Ca_Cd_df['daily_Ca'] = zip(*Ca_Cd_df.apply(lambda row: chill_days(row, Tc = Tc), axis = 1))\n",
    "    \n",
    "    ## Compute Ca_cumsum (a.k.a AGDD) and Cd_cumsum\n",
    "    years = Ca_Cd_df['year'].unique()\n",
    "    # years = [1992, 1993, 1994]\n",
    "    for yr in years:\n",
    "        # yr = years[1]\n",
    "        # print(yr)\n",
    "        Rc_start = datetime.strptime(str(int(yr) - 1) + \"-09-30\", \"%Y-%m-%d\")\n",
    "        \n",
    "        sub_df = Ca_Cd_df.loc[(Rc_start < Ca_Cd_df[\"date\"]) & (Ca_Cd_df[\"date\"] < datetime.strptime(str(yr)+\"-06-01\", \"%Y-%m-%d\")), :].reset_index(drop = True)\n",
    "    \n",
    "        list_id = station_id + \"-\" + str(yr)\n",
    "\n",
    "        if len(sub_df['month'].unique()) != 8:\n",
    "            # print(\"next\")\n",
    "            continue            \n",
    "\n",
    "        sub_df['Cd_cumsum'] = sub_df['daily_Cd'].cumsum()\n",
    "\n",
    "        if (np.isin(\"prcp\", sub_df.columns)):\n",
    "            sub_df[\"prcp_cumsum\"] = sub_df[\"prcp\"].cumsum()\n",
    "        \n",
    "        sub_df['Ca_cumsum'] = 0\n",
    "\n",
    "        if sub_df[sub_df['Cd_cumsum'] < Rc_thresh].shape[0] == 0:\n",
    "            continue\n",
    "        \n",
    "        Rc_thresh_loc = sub_df[sub_df['Cd_cumsum'] < Rc_thresh].index[0]\n",
    "\n",
    "        if pd.isna(Rc_thresh_loc):\n",
    "            Rc_thresh_loc = sub_df[sub_df['Cd_cumsum'] < Rc_thresh].index[0]\n",
    "            if pd.isna(Rc_thresh_loc):\n",
    "                continue\n",
    "\n",
    "        Rc_thresh_day = sub_df.iloc[Rc_thresh_loc]['date']\n",
    "        # print(paste0(\"reaches the Rc threshold on \", Rc_thresh_day)) # 저온요구도 달성일 i.e., 내생휴면 해제일. \n",
    "\n",
    "        if int(Rc_thresh_day.timetuple().tm_yday) > 31:\n",
    "            first_Tc_reach_day = datetime.strptime(str(yr) + \"-01-31\", \"%Y-%m-%d\")\n",
    "        else:\n",
    "            sub_df_afterRc = sub_df.iloc[range(Rc_thresh_loc, sub_df.shape[0]), :].reset_index(drop = True)\n",
    "            first_Tc_reach_loc = sub_df_afterRc[sub_df_afterRc['tmax'] > Tc].index[0]\n",
    "            first_Tc_reach_day = sub_df_afterRc.iloc[first_Tc_reach_loc]['date']\n",
    "\n",
    "        if pd.isna(first_Tc_reach_day):\n",
    "            # print(\"is na first tc reach day\")\n",
    "            continue\n",
    "        \n",
    "        first_Tc_reach_loc2 = sub_df[sub_df[\"date\"] == first_Tc_reach_day].index[0] # Ca accumulates starting this day.\n",
    "        sub_df.loc[first_Tc_reach_loc2:sub_df.shape[0], \"Ca_cumsum\"] = sub_df.loc[first_Tc_reach_loc2:sub_df.shape[0], \"daily_Ca\"].cumsum()\n",
    "        \n",
    "        # sub_df[\"diff_Ca_Cd\"] = sub_df['daily_Ca'].abs() - sub_df['daily_Cd'].abs()\n",
    "        # sub_df[\"diff_Ca_Cd_cumsum\"] = sub_df['diff_Ca_Cd'].cumsum()\n",
    "        \n",
    "        sub_df = sub_df[sub_df['month'].isin([1,2,3,4,5])].reset_index(drop=True)\n",
    "\n",
    "        sub_df['daily_gdd'] = sub_df.apply(lambda row: compute_gdd(row), axis = 1)\n",
    "        sub_df['AGDD'] = sub_df['daily_gdd'].cumsum()\n",
    "        \n",
    "        output_list[list_id] = sub_df\n",
    "\n",
    "    if len(output_list) == 0:\n",
    "        return pd.DataFrame(columns = sub_df.columns)\n",
    "    elif len(output_list) == 1:\n",
    "        out_df = output_list[list(output_list.keys())[0]].dropna().reset_index(drop = True)\n",
    "    elif len(output_list) > 1:\n",
    "        out_df = pd.concat(output_list, axis = 0).dropna().reset_index(drop = True)\n",
    "    \n",
    "\n",
    "    # return(out_df)\n",
    "    \n",
    "    return out_df\n",
    "\n",
    "\n",
    "def generate_cgdds(temperature_df, st, Tc, Rc_thresh):\n",
    "    \n",
    "    # st = target_ids[3]\n",
    "    station_temp = temperature_df[temperature_df[\"id\"] == st]\n",
    "    city_name = station_temp.iloc[1][\"city\"]\n",
    "    # city_name\n",
    "    station_bloom_years = station_temp['year'].unique()\n",
    "     \n",
    "    sub_cds = compute_cgdd(station_df = station_temp, station_id = st, Rc_thresh = Rc_thresh, Tc=Tc)\n",
    "    \n",
    "    return sub_cds\n",
    "\n",
    "\n",
    "def generate_data(temperature_df, target_ids, Tc, Rc_thresh, pooling = False):\n",
    "    \n",
    "    args = [(temperature_df, id, Tc, Rc_thresh) for id in target_ids]\n",
    "        \n",
    "    if pooling == True:\n",
    "        n_cpus = 7\n",
    "        pool = Pool(processes = n_cpus)\n",
    "        \n",
    "        df = pd.concat(pool.starmap(generate_cgdds, args), axis = 0)\n",
    "        pool.close()\n",
    "        pool.join()\n",
    "    else:\n",
    "        df = pd.concat([generate_gdds(*arg) for arg in args], axis = 0)\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function for hyperparameter tuning with Optuna\n",
    "\n",
    "def callback(study, trial):\n",
    "    if study.best_trial.number == trial.number:\n",
    "        study.set_user_attr(key=\"best_booster\", value=trial.user_attrs[\"best_booster\"])\n",
    "\n",
    "\n",
    "def train_cherry_blossom(trial):\n",
    "    \n",
    "    # target_vars = ['Ca_cumsum', \"Cd_cumsum\", \"prcp_cumsum\", \"AGDD\", \"lat\", \"long\", \"alt\", \"is_bloom\", \"month\", \"year\"]\n",
    "    target_set = cherry_complete.reset_index(drop = True)\n",
    "    \n",
    "    # Train-val-test split\n",
    "    test_years = list(range(2020, 2024))\n",
    "    test_set = target_set.query(\"year in @test_years\")\n",
    "\n",
    "    # dc_test = test_set.loc[test_set['State'] == \"DC\", :]\n",
    "    # dc_test['Phenophase_Status'].value_counts()\n",
    "\n",
    "\n",
    "    target_set.loc[target_set['State'] == \"DC\", :]\n",
    "    train_set = target_set.query(\"year not in @test_years\").drop(columns = ['year', 'month', 'day', 'date', \"Species\"])\n",
    "    \n",
    "    # Perform under-sampling\n",
    "    train_false = train_set[train_set['Phenophase_Status'] == 0]\n",
    "    train_true = train_set[train_set['Phenophase_Status'] == 1]\n",
    "    # train_true.head()\n",
    "    sample_idx = np.random.choice(range(len(train_false)), size = 2*len(train_true), replace = False)\n",
    "    train_df = pd.concat([train_false.iloc[sample_idx], train_true], axis = 0).reset_index(drop = True)\n",
    "    # train_df.head()\n",
    "    \n",
    "    # train_df = train_set.copy()\n",
    "    X_train, X_val, y_train, y_val = train_test_split(train_df.drop(columns = [\"Phenophase_Status\"]), train_df[\"Phenophase_Status\"], test_size = 0.2, shuffle = True, stratify=train_df[\"Phenophase_Status\"])\n",
    "\n",
    "    dtrain = lgb.Dataset(X_train, label = y_train)\n",
    "    dval = lgb.Dataset(X_val, label = y_val, reference=dtrain)\n",
    "\n",
    "    param = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": [\"binary_logloss\", \"focal_loss\", 'mape'],\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        # \"num_boost_round\": 2000,\n",
    "        # \"early_stopping_round\": 400,\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-8, 0.1, log = True),\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "    }\n",
    "\n",
    "    lgb_fit = lgb.train(\n",
    "        params = param,\n",
    "        train_set = dtrain,\n",
    "        valid_sets = [dval, dtrain],\n",
    "        verbose_eval=False,\n",
    "        num_boost_round = 2000,\n",
    "        callbacks = [lgb.early_stopping(stopping_rounds=400)]\n",
    "    )\n",
    "\n",
    "    trial.set_user_attr(key=\"best_booster\", value=lgb_fit)\n",
    "\n",
    "\n",
    "    def compute_mae(cities, test_years):\n",
    "        # cities = [\"DC\", \"NY\", \"BC\"]\n",
    "        # test_set.columns\n",
    "\n",
    "        errors = []\n",
    "\n",
    "        for city in cities:\n",
    "            # city = \"DC\"\n",
    "            mae_set = test_set.query(\"State == @city\").reset_index(drop = True)\n",
    "            mae_years = mae_set.loc[mae_set['Phenophase_Status'] ==1, \"year\"].unique()\n",
    "            # test_years = [t_year for t_year in test_years if t_year in mae_years]\n",
    "            \n",
    "            for year in mae_years:\n",
    "                \n",
    "                # year = test_years[0]\n",
    "                pred_df = mae_set.loc[mae_set['year'] == year, :]\n",
    "                \n",
    "                pred_X = pred_df.drop(columns = ['year', 'month', 'day', 'date', \"Phenophase_Status\", \"Species\"])\n",
    "            \n",
    "                pred = lgb_fit.predict(pred_X)\n",
    "                \n",
    "                pred_df['doy'] = pred_df.loc[:, \"date\"].apply(lambda row: pd.Period(row, freq = \"D\").day_of_year)\n",
    "                max_prob = np.where(pred == np.max(pred))[0][0]\n",
    "                \n",
    "                pred_doy = pred_df.iloc[max_prob][\"doy\"]\n",
    "                \n",
    "                actual_doy = pred_df.loc[pred_df['Phenophase_Status'] == 1, \"doy\"].item()\n",
    "                \n",
    "                absolute_error = abs(pred_doy - actual_doy)\n",
    "                # print(absolute_error)\n",
    "                errors.append(absolute_error)\n",
    "        \n",
    "        # print(np.round(np.mean(errors), 3))\n",
    "        return np.round(np.mean(errors), 3)\n",
    "\n",
    "    mae = compute_mae(cities = [\"DC\", \"NY\"], test_years = test_years)\n",
    "    \n",
    "    return mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Load data\n",
    "\n",
    "* For the American cities, we do not compute chill day-based cumulative gdd. Only use AGDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Elevation_in_Meters</th>\n",
       "      <th>State</th>\n",
       "      <th>Species</th>\n",
       "      <th>Day_of_Year</th>\n",
       "      <th>Phenophase_Status</th>\n",
       "      <th>AGDD</th>\n",
       "      <th>Accum_Prcp</th>\n",
       "      <th>date</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-122.855499</td>\n",
       "      <td>45.4856</td>\n",
       "      <td>63</td>\n",
       "      <td>OR</td>\n",
       "      <td>emarginata</td>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>508.50</td>\n",
       "      <td>274.0</td>\n",
       "      <td>2010-03-04</td>\n",
       "      <td>2010</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-122.855499</td>\n",
       "      <td>45.4856</td>\n",
       "      <td>63</td>\n",
       "      <td>OR</td>\n",
       "      <td>emarginata</td>\n",
       "      <td>73</td>\n",
       "      <td>1</td>\n",
       "      <td>582.75</td>\n",
       "      <td>320.0</td>\n",
       "      <td>2010-03-14</td>\n",
       "      <td>2010</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-122.855499</td>\n",
       "      <td>45.4856</td>\n",
       "      <td>63</td>\n",
       "      <td>OR</td>\n",
       "      <td>emarginata</td>\n",
       "      <td>74</td>\n",
       "      <td>1</td>\n",
       "      <td>594.00</td>\n",
       "      <td>320.0</td>\n",
       "      <td>2010-03-15</td>\n",
       "      <td>2010</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-122.855499</td>\n",
       "      <td>45.4856</td>\n",
       "      <td>63</td>\n",
       "      <td>OR</td>\n",
       "      <td>emarginata</td>\n",
       "      <td>86</td>\n",
       "      <td>0</td>\n",
       "      <td>716.75</td>\n",
       "      <td>347.0</td>\n",
       "      <td>2010-03-27</td>\n",
       "      <td>2010</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-122.855499</td>\n",
       "      <td>45.4856</td>\n",
       "      <td>63</td>\n",
       "      <td>OR</td>\n",
       "      <td>emarginata</td>\n",
       "      <td>94</td>\n",
       "      <td>0</td>\n",
       "      <td>776.25</td>\n",
       "      <td>428.0</td>\n",
       "      <td>2010-04-04</td>\n",
       "      <td>2010</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Longitude  Latitude  Elevation_in_Meters State     Species  Day_of_Year  \\\n",
       "0 -122.855499   45.4856                   63    OR  emarginata           63   \n",
       "1 -122.855499   45.4856                   63    OR  emarginata           73   \n",
       "2 -122.855499   45.4856                   63    OR  emarginata           74   \n",
       "3 -122.855499   45.4856                   63    OR  emarginata           86   \n",
       "4 -122.855499   45.4856                   63    OR  emarginata           94   \n",
       "\n",
       "   Phenophase_Status    AGDD  Accum_Prcp        date  year  month  day  \n",
       "0                  1  508.50       274.0  2010-03-04  2010      3    4  \n",
       "1                  1  582.75       320.0  2010-03-14  2010      3   14  \n",
       "2                  1  594.00       320.0  2010-03-15  2010      3   15  \n",
       "3                  0  716.75       347.0  2010-03-27  2010      3   27  \n",
       "4                  0  776.25       428.0  2010-04-04  2010      4    4  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(data_dir + \"/A31_america_temperatures2.csv\", \"r\") as file:\n",
    "    cherry_complete = pd.read_csv(file)\n",
    "\n",
    "for c in cherry_complete.columns:\n",
    "    col_type = cherry_complete[c].dtype\n",
    "    if col_type == \"object\" or col_type.name == 'category':\n",
    "        cherry_complete[c] = cherry_complete[c].astype('category')\n",
    "\n",
    "cherry_complete.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2c. Run hyperparameter optimization using Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-28 01:52:10,172] A new study created in memory with name: no-name-95b4103e-0767-4fe3-9aff-1f6fa63ed7bd\n",
      "/home/joosungm/venvs/py310/lib/python3.10/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/joosungm/venvs/py310/lib/python3.10/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 400 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[2000]\ttraining's binary_logloss: 0.206715\ttraining's mape: 0.148688\tvalid_0's binary_logloss: 0.264484\tvalid_0's mape: 0.17702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-28 01:52:16,289] Trial 0 finished with value: 13.667 and parameters: {'learning_rate': 0.0025266407527937307, 'lambda_l1': 1.1953870594782317, 'lambda_l2': 3.2117808761376444e-05, 'num_leaves': 87, 'feature_fraction': 0.4142461560707982, 'bagging_fraction': 0.9960698240098551, 'bagging_freq': 3, 'min_child_samples': 37}. Best is trial 0 with value: 13.667.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 2000 iterations\n",
      "Finished loading model, total used 2000 iterations\n",
      "Finished loading model, total used 2000 iterations\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joosungm/venvs/py310/lib/python3.10/site-packages/lightgbm/basic.py:1433: UserWarning: Overriding the parameters from Reference Dataset.\n",
      "  _log_warning('Overriding the parameters from Reference Dataset.')\n",
      "/home/joosungm/venvs/py310/lib/python3.10/site-packages/lightgbm/basic.py:1245: UserWarning: categorical_column in param dict is overridden.\n",
      "  _log_warning('{} in param dict is overridden.'.format(cat_alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 400 rounds\n",
      "Early stopping, best iteration is:\n",
      "[261]\ttraining's binary_logloss: 0.116541\ttraining's mape: 0.089726\tvalid_0's binary_logloss: 0.169\tvalid_0's mape: 0.11097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-28 01:52:19,463] Trial 1 finished with value: 11.833 and parameters: {'learning_rate': 0.05675954817395919, 'lambda_l1': 4.315524043154054e-07, 'lambda_l2': 1.5695043932486598e-06, 'num_leaves': 115, 'feature_fraction': 0.7984126995839793, 'bagging_fraction': 0.5706073256108849, 'bagging_freq': 3, 'min_child_samples': 41}. Best is trial 1 with value: 11.833.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 261 iterations\n",
      "Finished loading model, total used 261 iterations\n",
      "Finished loading model, total used 261 iterations\n",
      "Finished loading model, total used 261 iterations\n",
      "america: \n",
      "\n",
      "Finished loading model, total used 2000 iterations\n",
      "Finished loading model, total used 261 iterations\n",
      "Number of finished trials: 2 \n",
      "\n",
      "Best trial: \n",
      "Finished loading model, total used 261 iterations\n",
      "  Value: 11.833 \n",
      "\n",
      "  Params: \n",
      "    learning_rate: 0.05675954817395919\n",
      "    lambda_l1: 4.315524043154054e-07\n",
      "    lambda_l2: 1.5695043932486598e-06\n",
      "    num_leaves: 115\n",
      "    feature_fraction: 0.7984126995839793\n",
      "    bagging_fraction: 0.5706073256108849\n",
      "    bagging_freq: 3\n",
      "    min_child_samples: 41\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(\n",
    "    train_cherry_blossom, \n",
    "    n_trials=10000,\n",
    "    callbacks=[callback]\n",
    "    )\n",
    "# best_model=study.user_attrs[\"best_booster\"]\n",
    "best_model=study.best_trial.user_attrs[\"best_booster\"]\n",
    "\n",
    "# save the best model\n",
    "with open(model_dir + \"/B31_lgb_america.pkl\", 'wb') as model:\n",
    "    pickle.dump(best_model, model)\n",
    "\n",
    "# save the study\n",
    "with open(model_dir + \"/B31_study_america.pkl\", 'wb') as st:\n",
    "    pickle.dump(study, st)\n",
    "\n",
    "print(\"america: \\n\")\n",
    "\n",
    "print(\"Number of finished trials: {} \\n\".format(len(study.trials)))\n",
    "\n",
    "print(\"Best trial: \")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: {} \\n\".format(trial.value))\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
